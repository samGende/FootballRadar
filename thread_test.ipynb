{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Threading & Proccesing Speed Test\n",
    "\n",
    "In a recent interview, I was asked how to read in multiple large JSON files. My mind immediately went to distributed computing, such as PySpark. After further consideration, I realized that there are also ways to speed up this process even on a single instance. In this notebook we'll take a look at how we can use multithreading and multiprocessing to speed up reading json files. Using these two methods we can parrallise and read in multiple files at once. This speeds up the proccess of reading in multiple large files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading one json file took 0.10232782363891602 seconds\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import threading\n",
    "import multiprocessing\n",
    "\n",
    "# read the same json file that is about 30 MB \n",
    "file_list = [\"./player_data.json\", \"./player_data2.json\", \"./player_data3.json\", \"./player_data4.json\", \"./player_data5.json\"]\n",
    "\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = json.loads(file.read())\n",
    "    return content\n",
    "\n",
    "start = time.time()\n",
    "read_file(file_list[0])\n",
    "end = time.time()\n",
    "print(f'reading one json file took {end - start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in the json list took 0.41242122650146484 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def simple_reader(file_paths):\n",
    "    results = {}\n",
    "    for file in file_list:\n",
    "        results[file] = read_file(file)\n",
    "    return results\n",
    "    \n",
    "start= time.time()\n",
    "simple_reader(file_list)\n",
    "end = time.time()\n",
    "\n",
    "print(f'Reading in the json list took {end - start} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in 5 files took about five times as long as reading a single file, which is to be expected since we are reading the same file five times consecutively. To speed this up, we can try using multithreading to read the files concurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi threaded read took 0.4068269729614258 seconds\n"
     ]
    }
   ],
   "source": [
    "def multi_threaded_reader(file_paths):\n",
    "    threads = []\n",
    "    results = {}\n",
    "\n",
    "    def read_file_thread(file_path):\n",
    "        results[file_path] = read_file(file_path)\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        thread = threading.Thread(target=read_file_thread, args=(file_path,))\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "    \n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    \n",
    "    return results\n",
    "\n",
    "start = time.time()\n",
    "multi_threaded_reader(file_list)\n",
    "end = time.time()\n",
    "print(f'multi threaded read took {end - start} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that's not what we wanted multithreading took just as long. If we look at the docs, there's an issue with how the Python interpreter works. Here are the multithreading docs:\n",
    "\n",
    "\"CPython implementation detail: In CPython, due to the Global Interpreter Lock, only one thread can execute Python code at once (even though certain performance-oriented libraries might overcome this limitation).\"\n",
    "\n",
    "To avoid this lock, we can use multiprocessing, which allows us to start multiple processes. Each of these processes has its own Python interpreter, meaning we can run concurrent threads in separate processes. This requires more overhead than multithreading, and it is also more difficult to share data between processes than between threads. However, for reading large files, the advantage of concurrency is worth the tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiproccessing read took 0.10682511329650879 seconds\n"
     ]
    }
   ],
   "source": [
    "def multi_proccess_read(file_paths):\n",
    "    procceses = []\n",
    "    results = {}\n",
    "\n",
    "    def read_file_procces(file_path):\n",
    "        results[file_path] = read_file(file_path)\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        proccess = multiprocessing.Process(target=read_file_procces, args=(file_path,))\n",
    "        procceses.append(proccess)\n",
    "        proccess.start()\n",
    "    for proccess in procceses:\n",
    "        proccess.join()\n",
    "\n",
    "multiprocessing.set_start_method('fork', force=True)\n",
    "start = time.time()\n",
    "multi_proccess_read(file_list)\n",
    "end = time.time()\n",
    "print(f'multiproccessing read took {end - start} seconds')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is a much better runtime, cutting down the time closer to reading a single file. By using this strategy of multiprocessing, we can increase the throughput of reading files. This strategy works on a single instance of a machine. If we want to further speed up processing, we can use something like PySpark to leverage multiple machines, further increasing parallelization and throughput."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
